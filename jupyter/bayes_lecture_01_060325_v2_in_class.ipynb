{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bayesian thinking and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes's Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already derived Bayes's Rule:\n",
    "\n",
    "$$P(A|B) = \\frac{P(A) P(B|A)}{P(B)}$$\n",
    "\n",
    "As an example, we used an oil drilling example and Bayes's Rule  to compute conditional probability of finding oil if the seismic indicated that the prospect holds oil. We could also compute the probability of finding oil if the seismic indicated that the prospect **does not** hold oil as well as the probabilities of the prospect **not holding oil** if the seismic indicated \"oil\" or \"dry\" (not oil). \n",
    "\n",
    "In this chapter, we'll use it to solve several more challenging problems using conditional probability in the form of Bayes Rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cookie Problem\n",
    "\n",
    "<img src=\"../figs/cookie_problem.png\" alt=\"Alternative Text\" width=\"300\"/>\n",
    "\n",
    "We'll start with a simple case:\n",
    "\n",
    "> Suppose there are two bowls of cookies.\n",
    ">\n",
    "> * Bowl 1 contains 30 vanilla cookies and 10 chocolate cookies. \n",
    ">\n",
    "> * Bowl 2 contains 20 vanilla cookies and 20 chocolate cookies.\n",
    ">\n",
    "> Now suppose you choose one of the bowls at random and, without looking, choose a cookie at random. If the cookie is vanilla, what is the probability that it came from Bowl 1?\n",
    "\n",
    "What we want is the conditional probability that we chose from Bowl 1 given that we got a vanilla cookie, $P(B_1 | V)$.\n",
    "\n",
    "But what we get from the statement of the problem is:\n",
    "\n",
    "* The conditional probability of getting a vanilla cookie, given that we chose from Bowl 1, $P(V | B_1)$ and\n",
    "\n",
    "* The conditional probability of getting a vanilla cookie, given that we chose from Bowl 2, $P(V | B_2)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes's Rule tells us how they are related:\n",
    "\n",
    "$$P(B_1|V) = \\frac{P(B_1)~P(V|B_1)}{P(V)}$$\n",
    "\n",
    "The term on the left is what we want. The terms on the right are:\n",
    "\n",
    "-   $P(B_1)$, the probability that we chose Bowl 1,\n",
    "    unconditioned by what kind of cookie we got. \n",
    "    Since the problem says we chose a bowl at random, \n",
    "    we set $P(B_1) = 1/2$.\n",
    "\n",
    "-   $P(V|B_1)$, the probability of getting a vanilla cookie\n",
    "    from Bowl 1, which is 3/4.\n",
    "\n",
    "-   $P(V)$, the probability of drawing a vanilla cookie from\n",
    "    either bowl. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute $P(V)$, we can use the law of total probability:\n",
    "\n",
    "$$P(V) = P(B_1)~P(V|B_1) ~+~ P(B_2)~P(V|B_2)$$\n",
    "\n",
    "Plugging in the numbers from the statement of the problem, we have\n",
    "\n",
    "$$P(V) = (1/2)~(3/4) ~+~ (1/2)~(1/2) = 5/8$$\n",
    "\n",
    "We can also compute this result directly, like this: \n",
    "\n",
    "* Since we had an equal chance of choosing either bowl and the bowls contain the same number of cookies, we had the same chance of choosing any cookie. \n",
    "\n",
    "* Between the two bowls there are 50 vanilla and 30 chocolate cookies, so $P(V) = 5/8$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can apply Bayes's Rule to compute the posterior probability of Bowl 1:\n",
    "\n",
    "$$P(B_1|V) = (1/2)~(3/4)~/~(5/8) = 3/5$$\n",
    "\n",
    "This example demonstrates one use of Bayes's theorem: it provides a\n",
    "way to get from $P(B|A)$ to $P(A|B)$. \n",
    "This strategy is useful in cases like this where it is easier to compute the terms on the right side than the term on the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diachronic Bayes\n",
    "\n",
    "There is another way to think of Bayes's theorem: it gives us a way to\n",
    "update the probability of a hypothesis, $H$, given some body of data, $D$.\n",
    "\n",
    "This interpretation is \"diachronic\", which means \"related to change over time\"; in this case, the probability of the hypotheses changes as we see new data.\n",
    "\n",
    "Rewriting Bayes's rule with $H$ and $D$ yields:\n",
    "\n",
    "$$P(H|D) = \\frac{P(H)~P(D|H)}{P(D)}$$\n",
    "\n",
    "In this interpretation, each term has a name:\n",
    "\n",
    "-  $P(H)$ is the probability of the hypothesis before we see the data, called the prior probability, or just **prior**.\n",
    "\n",
    "-  $P(H|D)$ is the probability of the hypothesis after we see the data, called the **posterior**.\n",
    "\n",
    "-  $P(D|H)$ is the probability of the data under the hypothesis, called the **likelihood**.\n",
    "\n",
    "-  $P(D)$ is the **total probability of the data**, under any hypothesis.\n",
    "\n",
    "Sometimes we can compute the prior based on background information. For example, the cookie problem specifies that we choose a bowl at random with equal probability.\n",
    "\n",
    "In other cases the prior is subjective; that is, reasonable people might disagree, either because they use different background information or because they interpret the same information differently.\n",
    "\n",
    "The likelihood is usually the easiest part to compute. In the cookie\n",
    "problem, we are given the number of cookies in each bowl, so we can compute the probability of the data under each hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the total probability of the data can be tricky. \n",
    "It is the probability of seeing the data under any hypothesis at all. \n",
    "Most often we work through this by specifying a set of hypotheses that\n",
    "are:\n",
    "\n",
    "* Mutually exclusive, which means that only one of them can be true, and\n",
    "\n",
    "* Collectively exhaustive, which means one of them must be true.\n",
    "\n",
    "When these conditions apply, we can compute $P(D)$ using the law of total probability.  For example, with two hypotheses, $H_1$ and $H_2$:\n",
    "\n",
    "$$P(D) = P(H_1)~P(D|H_1) + P(H_2)~P(D|H_2)$$\n",
    "\n",
    "And more generally, with any number of hypotheses:\n",
    "\n",
    "$$P(D) = \\sum_i P(H_i)~P(D|H_i)$$\n",
    "\n",
    "The process in this section, using data and a prior probability to compute a posterior probability, is called a **Bayesian update**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Tables\n",
    "\n",
    "A convenient tool for doing a Bayesian update is a Bayes table.\n",
    "You can write a Bayes table on paper or use a spreadsheet, but in this section I'll use a Pandas `DataFrame`.\n",
    "\n",
    "First I'll make empty `DataFrame` with one row for each hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T19:35:10.584866Z",
     "iopub.status.busy": "2021-04-16T19:35:10.584115Z",
     "iopub.status.idle": "2021-04-16T19:35:10.819014Z",
     "shell.execute_reply": "2021-04-16T19:35:10.818427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prior</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bowl 1</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bowl 2</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Prior\n",
       "Bowl 1    0.5\n",
       "Bowl 2    0.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Define the hypotheses (choosing from Bowl 1 or Bowl 2)\n",
    "hypotheses = [\"Bowl 1\", \"Bowl 2\"]\n",
    "# Create an empty DataFrame with one row per hypothesis\n",
    "bayes_table = pd.DataFrame(index=hypotheses)\n",
    "# Add prior probabilities (P(H))\n",
    "bayes_table[\"Prior\"] = [1/2, 1/2]\n",
    "bayes_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll add a column to represent the priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T19:35:10.830244Z",
     "iopub.status.busy": "2021-04-16T19:35:10.829806Z",
     "iopub.status.idle": "2021-04-16T19:35:10.834513Z",
     "shell.execute_reply": "2021-04-16T19:35:10.834864Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a column for the likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T19:35:10.841661Z",
     "iopub.status.busy": "2021-04-16T19:35:10.840937Z",
     "iopub.status.idle": "2021-04-16T19:35:10.843527Z",
     "shell.execute_reply": "2021-04-16T19:35:10.843881Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a difference from the previous method: we compute likelihoods for both hypotheses, not just Bowl 1:\n",
    "\n",
    "* The chance of getting a vanilla cookie from Bowl 1 is 3/4.\n",
    "\n",
    "* The chance of getting a vanilla cookie from Bowl 2 is 1/2.\n",
    "\n",
    "As you see, the likelihoods don't add up to 1.  That's OK; each of them is a probability conditioned on a different hypothesis.\n",
    "There's no reason they should add up to 1 and no problem if they don't.\n",
    "\n",
    "The next step is similar to what we did with Bayes's Rule; we multiply the priors by the likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T19:35:10.847394Z",
     "iopub.status.busy": "2021-04-16T19:35:10.846784Z",
     "iopub.status.idle": "2021-04-16T19:35:10.856472Z",
     "shell.execute_reply": "2021-04-16T19:35:10.856847Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I call the result `unnorm` because these values are the \"unnormalized posteriors\".  Each of them is the product of a prior and a likelihood:\n",
    "\n",
    "$$P(H_i)~P(D|H_i)$$\n",
    "\n",
    "which is the numerator of Bayes's Rule. \n",
    "If we add them up, we have\n",
    "\n",
    "$$P(H_1)~P(D|H_1) + P(H_2)~P(D|H_2)$$\n",
    "\n",
    "which is the denominator of Bayes's Rule, $P(D)$.\n",
    "\n",
    "So we can compute the total probability of the data like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T19:35:10.860857Z",
     "iopub.status.busy": "2021-04-16T19:35:10.860423Z",
     "iopub.status.idle": "2021-04-16T19:35:10.863717Z",
     "shell.execute_reply": "2021-04-16T19:35:10.864069Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we get 5/8, which is what we got by computing $P(D)$ directly.\n",
    "\n",
    "And we can compute the posterior probabilities like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T19:35:10.872145Z",
     "iopub.status.busy": "2021-04-16T19:35:10.871531Z",
     "iopub.status.idle": "2021-04-16T19:35:10.875239Z",
     "shell.execute_reply": "2021-04-16T19:35:10.874623Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior probability for Bowl 1 is 0.6, which is what we got using Bayes's Rule explicitly.\n",
    "As a bonus, we also get the posterior probability of Bowl 2, which is 0.4.\n",
    "\n",
    "When we add up the unnormalized posteriors and divide through, we force the posteriors to add up to 1.  This process is called \"normalization\", which is why the total probability of the data is also called the \"normalizing constant\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Newspaper Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a Bayes table to solve the newspaper problem we discussed in the PowerPoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is Saturday morning at 08:00, and I must decide whether to walk down to the bottom of my driveway to get the newspaper.\n",
    "\n",
    ">On the basis of past experience, I judge that there is an 80% chance that the paper has been delivered by now.\n",
    "Looking out of the kitchen window, I can see exactly half of the bottom of the driveway, and the paper is not in the half that I see.\n",
    ">\n",
    ">If the paper has been delivered there’s an equal chance that it will fall in each half of the driveway.\n",
    ">\n",
    ">What is the probability that the paper has been delivered?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\n",
    "\n",
    "- D = Delivered\n",
    "\n",
    "- S = See the newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unnormalized posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total probability (the denominator in Bayes) of not seeing the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dice Problem\n",
    "\n",
    "<img src=\"../figs/Dice-2.png\" alt=\"Alternative Text\" width=\"200\"/>\n",
    "\n",
    "A Bayes table can also solve problems with more than two hypotheses.  For example:\n",
    "\n",
    "> Suppose I have a box with a 6-sided die, an 8-sided die, and a 12-sided die. I choose one of the dice at random, roll it, and report that the outcome is a 1. What is the probability that I chose the 6-sided die?\n",
    "\n",
    "In this example, there are three hypotheses with equal prior\n",
    "probabilities. The data is my report that the outcome is a 1. \n",
    "\n",
    "If I chose the 6-sided die, the probability of the data is\n",
    "1/6. If I chose the 8-sided die, the probability is 1/8, and if I chose the 12-sided die, it's 1/12.\n",
    "\n",
    "Here's a Bayes table that uses integers to represent the hypotheses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T19:35:10.879515Z",
     "iopub.status.busy": "2021-04-16T19:35:10.878981Z",
     "iopub.status.idle": "2021-04-16T19:35:10.881188Z",
     "shell.execute_reply": "2021-04-16T19:35:10.880775Z"
    }
   },
   "outputs": [],
   "source": [
    "table2 = pd.DataFrame(index=[6, 8, 12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use fractions to represent the prior probabilities and the likelihoods.  That way they don't get rounded off to floating-point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T19:35:10.886867Z",
     "iopub.status.busy": "2021-04-16T19:35:10.886335Z",
     "iopub.status.idle": "2021-04-16T19:35:10.894831Z",
     "shell.execute_reply": "2021-04-16T19:35:10.894457Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have priors and likelhoods, the remaining steps are always the same, so let's put them in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T19:35:10.898394Z",
     "iopub.status.busy": "2021-04-16T19:35:10.897966Z",
     "iopub.status.idle": "2021-04-16T19:35:10.899624Z",
     "shell.execute_reply": "2021-04-16T19:35:10.899989Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And call it like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T19:35:10.904682Z",
     "iopub.status.busy": "2021-04-16T19:35:10.904149Z",
     "iopub.status.idle": "2021-04-16T19:35:10.905883Z",
     "shell.execute_reply": "2021-04-16T19:35:10.906265Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the final Bayes table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T19:35:10.913780Z",
     "iopub.status.busy": "2021-04-16T19:35:10.913058Z",
     "iopub.status.idle": "2021-04-16T19:35:10.916229Z",
     "shell.execute_reply": "2021-04-16T19:35:10.915856Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior probability of the 6-sided die is 4/9, which is a little more than the probabilities for the other dice, 3/9 and 2/9.\n",
    "Intuitively, the 6-sided die is the most likely because it had the highest likelihood of producing the outcome we saw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Monty Hall Problem\n",
    "\n",
    "<img src=\"../figs/Monty_open_door.png\" alt=\"Alternative Text\" width=\"300\"/>\n",
    "\n",
    "Next we'll use a Bayes table to solve one of the most contentious problems in probability.\n",
    "\n",
    "The Monty Hall problem is based on a game show called *Let's Make a Deal*. If you are a contestant on the show, here's how the game works:\n",
    "\n",
    "* The host, Monty Hall, shows you three closed doors -- numbered 1, 2, and 3 -- and tells you that there is a prize behind each door.\n",
    "\n",
    "* One prize is valuable (traditionally a car), the other two are less valuable (traditionally goats).\n",
    "\n",
    "* The object of the game is to guess which door has the car. If you guess right, you get to keep the car.\n",
    "\n",
    "Suppose you pick Door 1. Before opening the door you chose, Monty opens Door 3 and reveals a goat. Then Monty offers you the option to stick with your original choice or switch to the remaining unopened door."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maximize your chance of winning the car, should you stick with Door 1 or switch to Door 2?\n",
    "\n",
    "To answer this question, we have to make some assumptions about the behavior of the host:\n",
    "\n",
    "1.  Monty always opens a door and offers you the option to switch.\n",
    "\n",
    "2.  He never opens the door you picked or the door with the car.\n",
    "\n",
    "3.  If you choose the door with the car, he chooses one of the other\n",
    "    doors at random.\n",
    "\n",
    "Under these assumptions, you are better off switching. \n",
    "If you stick, you win $1/3$ of the time. If you switch, you win $2/3$ of the time.\n",
    "\n",
    "If you have not encountered this problem before, you might find that\n",
    "answer surprising. You would not be alone; many people have the strong\n",
    "intuition that it doesn't matter if you stick or switch. There are two\n",
    "doors left, they reason, so the chance that the car is behind Door A is 50%. But that is wrong.\n",
    "\n",
    "To see why, it can help to use a Bayes table. We start with three\n",
    "hypotheses: the car might be behind Door 1, 2, or 3. According to the\n",
    "statement of the problem, the prior probability for each door is 1/3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T19:35:10.923035Z",
     "iopub.status.busy": "2021-04-16T19:35:10.922476Z",
     "iopub.status.idle": "2021-04-16T19:35:10.925144Z",
     "shell.execute_reply": "2021-04-16T19:35:10.925496Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is that Monty opened Door 3 and revealed a goat. So let's\n",
    "consider the probability of the data under each hypothesis (remember that you have chosen Door 1, but not opened it, so Monty will not open Door 1):\n",
    "\n",
    "* If the car is behind Door 1, Monty chooses Door 2 or 3 at random, so the probability he opens Door 3 is $1/2$.\n",
    "\n",
    "* If the car is behind Door 2, Monty has to open Door 3, so the probability of the data under this hypothesis is 1.\n",
    "\n",
    "* If the car is behind Door 3, Monty does not open it, so the probability of the data under this hypothesis is 0.\n",
    "\n",
    "Here are the likelihoods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T19:35:10.931993Z",
     "iopub.status.busy": "2021-04-16T19:35:10.931536Z",
     "iopub.status.idle": "2021-04-16T19:35:10.933829Z",
     "shell.execute_reply": "2021-04-16T19:35:10.934196Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have priors and likelihoods, we can use `update` to compute the posterior probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-16T19:35:10.944379Z",
     "iopub.status.busy": "2021-04-16T19:35:10.943730Z",
     "iopub.status.idle": "2021-04-16T19:35:10.946709Z",
     "shell.execute_reply": "2021-04-16T19:35:10.947151Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Monty opens Door 3, the posterior probability of Door 1 is $1/3$;\n",
    "the posterior probability of Door 2 is $2/3$.\n",
    "So you are better off switching from Door 1 to Door 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little more discussion that, hopefully, provides more insight regarding the Monty Hall problem.\n",
    "\n",
    "How **Monty’s Choice Gives Information**\n",
    "\n",
    "- Monty always opens a door that is NOT the one you picked and NOT the car.\n",
    "\n",
    "- If the car were behind Door 1, Monty could choose either Door 2 or Door 3 at random.\n",
    "\n",
    "- If the car were behind Door 2, Monty is forced to open Door 3.\n",
    "\n",
    "- If the car were behind Door 3, Monty is forced to open Door 2.\n",
    "\n",
    "This means that some of Monty’s choices are more likely under certain conditions, which allows us to update our probabilities using Bayes’ Rule.\n",
    "\n",
    "**Key Insight from the Information Update**\n",
    "\n",
    "- **Obvious Information**: When Monty opens Door 3, we know for certain that the car is **not behind Door 3**.\n",
    "\n",
    "- **Less Obvious Information**: Monty’s action is **not independent** of where the car is.\n",
    "\n",
    "1. He was forced to open Door 3 if the car was behind Door 2.\n",
    "2. If the car was behind Door 1, he had a choice between opening Door 2 or Door 3.\n",
    "3. Because he opens Door 3, this slightly reduces the likelihood that the car was behind Door 1, and increases the likelihood that it is behind Door 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this example shows, our intuition for probability is not always\n",
    "reliable. \n",
    "Bayes's Rule can help by providing a divide-and-conquer strategy:\n",
    "\n",
    "1.  First, write down the hypotheses and the data.\n",
    "\n",
    "2.  Next, figure out the prior probabilities.\n",
    "\n",
    "3.  Finally, compute the likelihood of the data under each hypothesis.\n",
    "\n",
    "The Bayes table does the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We solved the Cookie Problem using Bayes's Rule explicitly and using a Bayes table.\n",
    "There's no real difference between these methods, but the Bayes table can make it easier to compute the total probability of the data, especially for problems with more than two hypotheses.\n",
    "\n",
    "Then we solved the Dice Problem, which we will see again later, and the Monty Hall problem, which you might hope you never see again 😊.\n",
    "\n",
    "If the Monty Hall problem makes your head hurt, you are not alone.  But it demonstrates the power of Bayes's Rule as a divide-and-conquer strategy for solving tricky problems.  And I hope it provides some insight into *why* the answer is what it is.\n",
    "\n",
    "When Monty opens a door, he provides information we can use to update our belief about the location of the car.  Part of the information is obvious.  If he opens Door 3, we know the car is not behind Door 3.  But part of the information is more subtle.  Opening Door 3 is more likely if the car is behind Door 2, and less likely if it is behind Door 1.  So the data is evidence in favor of Door 2.  We will come back to this notion of evidence in future chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1741180389989,
   "trusted": false
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
