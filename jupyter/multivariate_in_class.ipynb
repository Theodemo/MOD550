{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Multivariate Analysis for Spatial Data Analytics in Python \n",
    "\n",
    "\n",
    "### Reidar B Bratvold,  Professor, University of Stavanger "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate Analysis for Subsurface Data Analytics in Python \n",
    "\n",
    "Here's a simple workflow, demonstration of multivariate analysis for subsurface modeling workflows. This should help you get started with building subsurface models that integrate uncertainty in the sample statistics.  \n",
    "\n",
    "#### Bivariate Analysis\n",
    "\n",
    "Understand and quantify the relationship between two variables\n",
    "\n",
    "* example: relationship between porosity and permeability\n",
    "* how can we use this relationship?\n",
    "\n",
    "What would be the impact if we ignore this relationship and simply modeled porosity and permeability independently?\n",
    "\n",
    "* no relationship beyond constraints at data locations\n",
    "* independent away from data\n",
    "* nonphysical results, unrealistic uncertainty models\n",
    "\n",
    "#### Bivariate Statistics\n",
    "\n",
    "Pearson’s Product‐Moment Correlation Coefficient\n",
    "* Provides a measure of the degree of linear relationship.\n",
    "* We refer to it as the 'correlation coefficient'\n",
    "\n",
    "Let's review the sample variance of variable $x$. Of course, I'm truncating our notation as $x$ is a set of samples a locations in our modeling space, $x(\\bf{u_\\alpha}), \\, \\forall \\, \\alpha = 0, 1, \\dots, n - 1$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma^2_{x}  = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})^2}{(n-1)}\n",
    "\\end{equation}\n",
    "\n",
    "We can expand the the squared term and replace on of them with $y$, another variable in addition to $x$.\n",
    "\n",
    "\\begin{equation}\n",
    "C_{xy}  = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{(n-1)}\n",
    "\\end{equation}\n",
    "\n",
    "We now have a measure that represents the manner in which variables $x$ and $y$ co-vary or vary together.  We can standardized the covariance by the product of the standard deviations of $x$ and $y$ to calculate the correlation coefficent. \n",
    "\n",
    "\\begin{equation}\n",
    "\\rho_{xy}  = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{(n-1)\\sigma_x \\sigma_y}, \\, -1.0 \\le \\rho_{xy} \\le 1.0\n",
    "\\end{equation}\n",
    "\n",
    "In summary we can state that the correlation coefficient is related to the covariance as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho_{xy}  = \\frac{C_{xy}}{\\sigma_x \\sigma_y}\n",
    "\\end{equation}\n",
    "\n",
    "The Pearson's correlation coefficient is quite sensitive to outliers and depature from linear behavoir (in the bivariate sense).  We have an altenrative known as the Spearman's rank correlations coefficient.   \n",
    "\n",
    "\\begin{equation}\n",
    "\\rho_{R_x R_y}  = \\frac{\\sum_{i=1}^{n} (R_{x_i} - \\overline{R_x})(R_{y_i} - \\overline{R_y})}{(n-1)\\sigma_{R_x} \\sigma_{R_y}}, \\, -1.0 \\le \\rho_{xy} \\le 1.0\n",
    "\\end{equation}\n",
    "\n",
    "The rank correlation applies the rank transform to the data prior to calculating the correlation coefficent.  To calculate the rank transform simply replace the data values with the rank $R_x = 1,\\dots,n$, where $n$ is the maximum value and $1$ is the minimum value. \n",
    "\n",
    "\\begin{equation}\n",
    "x_\\alpha, \\, \\forall \\alpha = 1,\\dots, n, \\, | \\, x_i \\ge x_j \\, \\forall \\, i \\gt j \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "R_{x_i} = i\n",
    "\\end{equation}\n",
    "\n",
    "The corelation coefficients provide useful metrics to quantify relationships between two variables at a time. We can also consider bivariate scatter plots and matrix scatter plots to visualize multivariate data. In general, current practical subsurface modeling is bivariate, two variables at a time.    \n",
    "\n",
    "#### Multivariate Statistics\n",
    "\n",
    "See lecture on Multivariate Statistics, including the concepts of joint, conditional and marginal probability.\n",
    "\n",
    "#### Objective \n",
    "\n",
    "To provide hands-on experience with building subsurface modeling workflows. Python provides an excellent vehicle to accomplish this. \n",
    "\n",
    "The objective is to remove the hurdles of subsurface modeling workflow construction by providing building blocks and sufficient examples. This is not a coding class per se, but we need the ability to 'script' workflows working with numerical methods.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some some standard packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                        # ndarrys for gridded data\n",
    "import pandas as pd                       # DataFrames for tabular data\n",
    "import os                                 # set working directory, run executables\n",
    "import matplotlib.pyplot as plt           # for plotting\n",
    "from scipy import stats                   # summary statistics\n",
    "import math                               # trig etc.\n",
    "import scipy.signal as signal             # kernel for moving window calculation\n",
    "import random\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Tabular Data\n",
    "\n",
    "Here's the command to load our comma delimited data file in to a Pandas' DataFrame object.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data_MV_biased.csv') # load our data table (wrong name!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the DataFrame would be useful and we already learned about these methods in this demo (https://git.io/fNgRW). \n",
    "\n",
    "We can preview the DataFrame by printing a slice or by utilizing the 'head' DataFrame member function (with a nice and clean format, see below). With the slice we could look at any subset of the data table and with the head command, add parameter 'n=13' to see the first 13 rows of the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0:5,:])                   # display first 4 samples in the table as a preview\n",
    "df.head(n=13)                           # we could also use this command for a table preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Statistics for Tabular Data\n",
    "\n",
    "The table includes X and Y coordinates (meters), Facies 1 and 0 (1 is sandstone and 0 interbedded sand and mudstone), Porosity (fraction), and permeability as Perm (mDarcy). \n",
    "\n",
    "There are a lot of efficient methods to calculate summary statistics from tabular data in DataFrames. The describe command provides count, mean, minimum, maximum, and quartiles all in a nice data table. We use transpose just to flip the table so that features are on the rows and the statistics are on the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Tabular Data with Location Maps \n",
    "\n",
    "It is natural to set the x and y coordinate and feature ranges manually. e.g. do you want your color bar to go from 0.05887 to 0.24230 exactly? Also, let's pick a color map for display. I heard that plasma is known to be friendly to the color blind as the color and intensity vary together (hope I got that right, it was an interesting Twitter conversation started by Matt Hall from Agile if I recall correctly). We will assume a study area of 0 to 1,000m in x and y and omit any data outside this area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 0.0; xmax = 1000.0               # range of x values\n",
    "ymin = 0.0; ymax = 1000.0               # range of y values\n",
    "pormin = 0.05; pormax = 0.25;           # range of porosity values\n",
    "permmin = 0.01; permmax = 2000.0         # range of permeability values\n",
    "AImin = 2000.0; AImax = 8000.0          # range of AI values\n",
    "nx = 100; ny = 100; csize = 10.0\n",
    "cmap = plt.cm.plasma                    # color map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out locmap. This is a reimplementation of GSLIB's locmap program that uses matplotlib. I hope you find it simpler than matplotlib, if you want to get more advanced and build custom plots lock at the source. If you improve it, send me the new code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can populate the plotting parameters and visualize the porosity data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create figure and subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Define color maps\n",
    "cmap = 'plasma'  # Try 'viridis', 'inferno', or 'coolwarm' for different styles\n",
    "marker_size = 10  # Adjust marker size\n",
    "\n",
    "# Plot 1: Facies Well Data\n",
    "sc = axes[0, 0].scatter(df['X'], df['Y'], c=df['Facies'], cmap=cmap, edgecolors='black', alpha=0.8, s=marker_size)\n",
    "axes[0, 0].set_title(\"Well Data - Facies\", fontsize=14)\n",
    "axes[0, 0].set_xlabel(\"X (m)\", fontsize=12)\n",
    "axes[0, 0].set_ylabel(\"Y (m)\", fontsize=12)\n",
    "axes[0, 0].set_aspect('equal', adjustable='box')  # Ensure square plot\n",
    "cb = plt.colorbar(sc, ax=axes[0, 0])\n",
    "cb.set_label(\"Facies (0=shale, 1=sand)\")\n",
    "\n",
    "# Plot 2: Porosity Well Data\n",
    "sc = axes[0, 1].scatter(df['X'], df['Y'], c=df['Porosity'], cmap=cmap, edgecolors='black', alpha=0.8, s=marker_size)\n",
    "axes[0, 1].set_title(\"Well Data - Porosity\", fontsize=14)\n",
    "axes[0, 1].set_xlabel(\"X (m)\", fontsize=12)\n",
    "axes[0, 1].set_ylabel(\"Y (m)\", fontsize=12)\n",
    "axes[0, 1].set_aspect('equal', adjustable='box')  # Square plot\n",
    "cb = plt.colorbar(sc, ax=axes[0, 1])\n",
    "cb.set_label(\"Porosity (fraction)\")\n",
    "\n",
    "# Plot 3: Permeability Well Data\n",
    "sc = axes[1, 0].scatter(df['X'], df['Y'], c=df['Perm'], cmap=cmap, edgecolors='black', alpha=0.8, s=marker_size)\n",
    "axes[1, 0].set_title(\"Well Data - Permeability\", fontsize=14)\n",
    "axes[1, 0].set_xlabel(\"X (m)\", fontsize=12)\n",
    "axes[1, 0].set_ylabel(\"Y (m)\", fontsize=12)\n",
    "axes[1, 0].set_aspect('equal', adjustable='box')  # Square plot\n",
    "cb = plt.colorbar(sc, ax=axes[1, 0])\n",
    "cb.set_label(\"Permeability (mD)\")\n",
    "\n",
    "# Plot 4: Acoustic Impedance Well Data\n",
    "sc = axes[1, 1].scatter(df['X'], df['Y'], c=df['AI'], cmap=cmap, edgecolors='black', alpha=0.8, s=marker_size)\n",
    "axes[1, 1].set_title(\"Well Data - Acoustic Impedance\", fontsize=14)\n",
    "axes[1, 1].set_xlabel(\"X (m)\", fontsize=12)\n",
    "axes[1, 1].set_ylabel(\"Y (m)\", fontsize=12)\n",
    "axes[1, 1].set_aspect('equal', adjustable='box')  # Square plot\n",
    "cb = plt.colorbar(sc, ax=axes[1, 1])\n",
    "cb.set_label(\"Acoustic Impedance (kg/m2s * 10^6)\")\n",
    "\n",
    "# Adjust layout for clarity\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bivariate Analysis\n",
    "\n",
    "Let's start with some simple bivariate plotting and calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.plot(df['Porosity'].values,df['Perm'].values, 'o', label='', markerfacecolor='red', markeredgecolor='black', alpha=0.2)\n",
    "plt.title('Well Data Permeability vs. Porostiy')\n",
    "plt.xlabel('Porosity (fraction)')\n",
    "plt.ylabel('Permeability (mD)')\n",
    "#plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(df['AI'].values,df['Porosity'].values, 'o', label='', markerfacecolor='red', markeredgecolor='black', alpha=0.2)\n",
    "plt.title('Well Data Porostiy vs. Acoustic Impedance')\n",
    "plt.ylabel('Porosity (fraction)')\n",
    "plt.xlabel('Acoustic Impedance (m/s x g/cm^3)')\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.2, top=1.2, wspace=0.2, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation and Covariance\n",
    "\n",
    "It is straight forward to calculate the covariance and correlation from the pairs of data in our dataset. Here's the covariance.  Notice that the matrix is symmetrical?  Makes sense, as the $C_{Por,Perm} = C_{Perm,Por}$.  Also, note that the diagonal values ($C_{i,j}$ where $i=j$) equal to the variance.  We check porosity by calculating the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute and print the covariance matrix for columns 3 to 6\n",
    "print(df.iloc[:, 3:7].cov())\n",
    "\n",
    "# Compute the variance of 'Porosity'\n",
    "porosity_variance = round(np.var(df['Porosity'].to_numpy()), 6)\n",
    "print(f\"The variance of porosity is {porosity_variance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the correlation coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,3:7].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Scatter Plots\n",
    "\n",
    "If we have 3 or more variables to consider then matrix scatter plot offer an efficient method to display the multivariate relationships, 2 variables at a time.  We can identify:\n",
    "\n",
    "1. the range, envelope of the paired data\n",
    "2. homoscedastic and heteroscedastic behavoirs\n",
    "3. non-linear features\n",
    "\n",
    "Here's the seaborn package matrix scatter plot function, pairplot. Let's color the results by facies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue='Facies',vars=['Facies','Porosity','Perm','AI'],markers='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Analysis of Seaborn Pairplot**\n",
    "\n",
    "#### **1. Identifying the Range and Envelope of the Paired Data**\n",
    "- **Range**: The axis limits of each scatter plot indicate the range of the variables:\n",
    "  - **Porosity:** ~0.05 to ~0.2\n",
    "  - **Perm (Permeability):** 0 to ~2000\n",
    "  - **AI (Acoustic Impedance):** ~2000 to ~8000\n",
    "- **Envelope (Data Spread and Bounds)**:\n",
    "  - Scatter plots show the **spread of values** between two variables.\n",
    "  - If data points are widely dispersed, the **envelope is broad**.\n",
    "  - If data points are tightly clustered, the **envelope is narrow**.\n",
    "  - **Density plots (KDEs) on the diagonal** provide insight into the distribution of each variable.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Identifying Homoscedastic and Heteroscedastic Behavior**\n",
    "- **Homoscedasticity (Constant Variance):**\n",
    "  - If scatter plots show an **even spread of data points across all values of X**, then the data is **homoscedastic**.\n",
    "  - Example: If **AI vs. Porosity** has a **consistent spread across all values**, it indicates **homoscedasticity**.\n",
    "\n",
    "- **Heteroscedasticity (Changing Variance):**\n",
    "  - If the **spread of data points increases or decreases** as the X variable changes, it suggests **heteroscedasticity**.\n",
    "  - Example: \n",
    "    - **Perm vs. Porosity**: Shows an **increasing spread of permeability values as porosity increases**, indicating **heteroscedasticity**.\n",
    "    - **AI vs. Porosity**: Displays a **funnel-like shape**, signaling a **variance shift**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Identifying Non-Linear Features**\n",
    "- **Curved or Clustered Relationships in Scatter Plots:**\n",
    "  - If data points **form a clear curve rather than a straight line**, this indicates a **non-linear relationship**.\n",
    "  - Example:\n",
    "    - **Perm vs. Porosity**: Shows a **non-linear trend**—as porosity increases, permeability increases in a **curved** rather than a straight-line fashion.\n",
    "    - **AI vs. Porosity**: Also suggests a **non-linear effect**.\n",
    "\n",
    "- **Divergence in KDE Distributions:**\n",
    "  - The **density plots (diagonal histograms/KDEs)** reveal how different variables distribute across the Facies (0 vs. 1).\n",
    "  - Example:\n",
    "    - **Porosity distributions differ significantly for the two facies**, highlighting a **non-linear effect**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **From the Pairplot:**\n",
    "- **Heteroscedastic behavior** is visible in **Perm vs. Porosity** (increasing variance).\n",
    "- **Non-linearity** is evident in **Perm vs. Porosity** and **AI vs. Porosity**.\n",
    "- **The range/envelope** of each variable can be observed from the axis limits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joint, Conditional and Marginals\n",
    "\n",
    "We can use kernel density estimation to estimate the joint probabilities density function (pdf) for the paired data, a 2D pdf! We could use this to estimate any required joint, marginal and conditional probability (care must be taken with normalization). Let's use the seaborn package 'kdeplot' function to estimate the joint pdf for porosity and acoustic impedance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define colormap (modify as needed)\n",
    "cmap = \"plasma\"\n",
    "\n",
    "# Create KDE plot using the correct syntax\n",
    "ax = sns.kdeplot(x=df['AI'].values, y=df['Porosity'].values, fill=True, levels=10, cmap=cmap)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Acoustic Impedance (m/s x g/cm^3)', fontsize=12)\n",
    "ax.set_ylabel('Porosity (fraction)', fontsize=12)\n",
    "ax.set_title('Porosity vs. Acoustic Impedance', fontsize=14)\n",
    "\n",
    "# Show color bar\n",
    "plt.colorbar(ax.collections[-1], label=\"Density\")  # Add colorbar manually\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KDE-plot shows that most of the datapoints, the high-density region, are in the center. The plot also shows a negative correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also useful to visualize the joint pdfs with the marginal pdfs on a single plot. We can use seaborn's 'jointgrid' to accomplish this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define colormap\n",
    "cmap = \"plasma\"\n",
    "\n",
    "# Prepare the data: drop NaNs, reset index, and ensure numeric types\n",
    "df = df.copy()\n",
    "df = df[['AI', 'Porosity']].dropna().reset_index(drop=True)\n",
    "df['AI'] = pd.to_numeric(df['AI'], errors='coerce')\n",
    "df['Porosity'] = pd.to_numeric(df['Porosity'], errors='coerce')\n",
    "\n",
    "# Convert columns to 1D NumPy arrays\n",
    "x = df[\"AI\"].values\n",
    "y = df[\"Porosity\"].values\n",
    "\n",
    "# Create a JointGrid and plot the joint KDE contour (without the marginals)\n",
    "g = sns.JointGrid(x=x, y=y, height=6)\n",
    "\n",
    "# Plot the joint KDE contour\n",
    "g.plot_joint(sns.kdeplot, levels=10, cmap=cmap, fill=False)\n",
    "\n",
    "# Optionally, remove the marginal plots if you don't need them\n",
    "g.ax_marg_x.set_visible(False)\n",
    "g.ax_marg_y.set_visible(False)\n",
    "\n",
    "# Adjust labels and title\n",
    "g.ax_joint.set_xlabel(\"Acoustic Impedance (m/s × g/cm³)\", fontsize=12)\n",
    "g.ax_joint.set_ylabel(\"Porosity (fraction)\", fontsize=12)\n",
    "g.ax_joint.set_title(\"Porosity vs. Acoustic Impedance\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient and the p-value of the correlation coefficient (significant if < $\\alpha/2$ or > $1-\\alpha/2$). \n",
    "\n",
    "#### Calculating Conditional Statistics\n",
    "\n",
    "Of course, we could just calculate the conditional statistics by-hand. We need to select some bins over the variable that we will condition to. Let's calculate conditional statistical of porosity given acoustic impedance. We will select 9 equal spaced bins.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_bins = np.linspace(2000,8000,10)            # set the bin boundaries and then the centroids for plotting\n",
    "AI_centroids = np.linspace((AI_bins[0]+AI_bins[1])*0.5,(AI_bins[8]+AI_bins[9])*0.5,9)\n",
    "print(AI_bins)                                 # check the boundaries\n",
    "print(AI_centroids)                            # check the centroids\n",
    "df['AI_bins'] = pd.cut(df['AI'], AI_bins,labels = AI_centroids) # cut on bondaries and lable with centroids \n",
    "df.head()                                      # check the new column in the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the 'groupby' function built-in to Pandas' DataFrames to extract subsets of porosity values in each bin from the DataFrame and then to calculate the conditional statistics: expectation, P90 and P10. Let's plot the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the conditional expectations and quantiles\n",
    "cond_exp = df.groupby('AI_bins')['Porosity'].mean()\n",
    "cond_P90 = df.groupby('AI_bins')['Porosity'].quantile(0.9)\n",
    "cond_P10 = df.groupby('AI_bins')['Porosity'].quantile(0.1)\n",
    "\n",
    "# Convert the results to 1D NumPy arrays\n",
    "cond_exp_arr = cond_exp.to_numpy()\n",
    "cond_P90_arr = cond_P90.to_numpy()\n",
    "cond_P10_arr = cond_P10.to_numpy()\n",
    "\n",
    "# Plot the results (ensure AI_centroids is also a 1D array or list)\n",
    "plt.figure()\n",
    "plt.plot(AI_centroids, cond_exp_arr, color='black', label='Expectation')\n",
    "plt.plot(AI_centroids, cond_P90_arr, 'r--', linewidth=1.0, label='P90')\n",
    "plt.plot(AI_centroids, cond_P10_arr, 'r--', linewidth=1.0, label='P10')\n",
    "\n",
    "plt.xlabel('Acoustic Impedance (m/s x g/cm^3)')\n",
    "plt.ylabel('Porosity (fraction)')\n",
    "plt.title('Porosity Conditional to Acoustic Impedance')\n",
    "plt.ylim(pormin, pormax)\n",
    "plt.xlim(AImin, AImax)\n",
    "\n",
    "plt.text(3200, .10, 'P10')\n",
    "plt.text(3200, .15, 'Expectation')\n",
    "plt.text(3200, .19, 'P90')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.2, top=1.2, wspace=0.2, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does acoustic impedance provide information about porosity? \n",
    "\n",
    "Yes, clearly the conditional statistics vary over acoustic impedance, knowing the acoustic impedance reduces  uncertainty about porosity.\n",
    "\n",
    "#### Comments\n",
    "\n",
    "This was a basic demonstration of multivariate analysis. A lot more could be done, for example, there are methods that reduce the dimensionality, and remove dependency to allow for independent variable modeling workflows etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1739874041530,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
