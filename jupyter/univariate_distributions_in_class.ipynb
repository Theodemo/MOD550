{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic Univariate Distributions in Python \n",
    "\n",
    "## Reidar B Bratvold, Professor, University of Stavanger \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Univariate Data Distribution Plotting in Python\n",
    "\n",
    "Here's a simple workflow with some basic univariate statistics and distribution plotting of tabular (easily extended to gridded) data summary statistics and distributions. This should help you get started data visualization and interpretation.\n",
    "\n",
    "#### Objective \n",
    "\n",
    "The objective is to illustrate univariate distributions   \n",
    "\n",
    "#### Getting Started\n",
    "\n",
    "You will need to copy the data files to your working directory.  They are avaiable in the same Colab directory as the Jupyter Notebook. A link has been provided on Canvas.\n",
    "\n",
    "- sample_data.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need some standard packages. These should have been installed with Anaconda 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                        # ndarrys for gridded data\n",
    "import pandas as pd                       # DataFrames for tabular data\n",
    "import os                                 # set working directory, run executables\n",
    "import matplotlib.pyplot as plt           # for plotting\n",
    "from scipy import stats                   # summary statistics\n",
    "import seaborn as sns                     # advanced plotting\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Tabular Data\n",
    "\n",
    "Here's the command to load our comma delimited data file in to a Pandas' DataFrame object.  For fun try misspelling the name. You will get an ugly, long error.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(os.getcwd())  # Print the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_data.csv')     # load our data table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loaded our file into our DataFrame called 'df'. But how do you really know that it worked? Visualizing the DataFrame would be useful and we already leard about these methods in this demo. \n",
    "\n",
    "We can preview the DataFrame by printing a slice or by utilizing the 'head' DataFrame member function (with a nice and clean format, see below). With the slice we could look at any subset of the data table and with the head command, add parameter 'n=13' to see the first 13 rows of the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[0:5,:])                   # display first 4 samples in the table as a preview\n",
    "df.head(n=13)                           # we could also use this command for a table preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Description**\n",
    "\n",
    "This dataset contains key geophysical and petrophysical properties used in reservoir characterization. Below is a brief description of each parameter:\n",
    "\n",
    "### **1. X, Y (Spatial Coordinates)**\n",
    "- Represents the spatial location of data points in the reservoir.\n",
    "- These coordinates help in mapping and visualization of subsurface properties.\n",
    "\n",
    "### **2. Facies**\n",
    "- A categorical variable representing different geological facies in the reservoir.\n",
    "- Typically, **Facies = 0** and **Facies = 1** indicate different lithological units (e.g., shale vs. sandstone).\n",
    "\n",
    "### **3. Porosity**\n",
    "- Denoted as a fraction (0 to 1), indicating the pore space in the rock.\n",
    "- Higher porosity means more storage capacity for fluids like oil, gas, or water.\n",
    "\n",
    "### **4. Permeability (Perm)**\n",
    "- Measured in millidarcies (mD), it quantifies the ability of fluids to flow through the rock.\n",
    "- Higher permeability values indicate better reservoir quality.\n",
    "\n",
    "### **5. Acoustic Impedance (AI)**\n",
    "- Defined as the product of **P-wave velocity** and **rock density**.\n",
    "- Used in seismic inversion to distinguish between different rock types and fluid contents.\n",
    "- Higher AI values suggest denser and/or faster materials (e.g., carbonates, tight sands), while lower AI values indicate more porous formations.\n",
    "\n",
    "This dataset can be used for **reservoir characterization, seismic interpretation, and petrophysical analysis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Univariate Statistics for Tabular Data\n",
    "\n",
    "The table includes X and Y coordinates (meters), Facies 1 and 2 (1 is sandstone and 0 interbedded sand and mudstone), Porosity (fraction), permeability as Perm (mDarcy) and acoustic impedance as AI (kg/m2s*10^6). \n",
    "\n",
    "There are a lot of efficient methods to calculate summary statistics from tabular data in DataFrames. The describe command provides count, mean, minimum, maximum, and quartiles all in a nice data table. We use transpose just to flip the table so that features are on the rows and the statistics are on the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a wide variety of statistical summaries built into NumPy's ndarrays.  When we use the command:\n",
    "```p\n",
    "df['Porosity']                       # returns an Pandas series\n",
    "df['Porosity'].values                # returns an ndarray\n",
    "```\n",
    "Panda's DataFrame returns all the porosity data as a series and if we add 'values' it returns a NumPy ndarray and we have access to a lot of NumPy methods. I also like to use the round function to round the answer to a limited number of digits for accurate reporting of precision and ease of reading.\n",
    "\n",
    "For example, now we could use commands. like this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The minimum is {df['Porosity'].min():.2f}.\")\n",
    "print(f\"The maximum is {df['Porosity'].max():.2f}.\")\n",
    "print(f\"The mean is {df['Porosity'].mean():.2f}.\")\n",
    "print(f\"The sample variance is {df['Porosity'].var():.6f}.\")\n",
    "print(f\"The standard deviation is {df['Porosity'].std():.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simetimes it is useful to calculate the variance as the Difference of Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of Sample Variance Using the Difference of Means\n",
    "\n",
    "The **sample variance** $s^2$ is defined as:\n",
    "\n",
    "$$\n",
    "s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x_i$ represents each sample value,\n",
    "- $\\bar{x}$ is the sample mean,\n",
    "- $n$ is the number of observations.\n",
    "\n",
    "#### **Step 1: Expand the Squared Terms**\n",
    "Using the identity:\n",
    "\n",
    "$$\n",
    "(x_i - \\bar{x})^2 = x_i^2 - 2\\bar{x}x_i + \\bar{x}^2\n",
    "$$\n",
    "\n",
    "Expanding the sum:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} (x_i - \\bar{x})^2 = \\sum_{i=1}^{n} x_i^2 - 2\\bar{x} \\sum_{i=1}^{n} x_i + \\sum_{i=1}^{n} \\bar{x}^2\n",
    "$$\n",
    "\n",
    "#### **Step 2: Use the Definition of the Mean**\n",
    "Since the mean is:\n",
    "\n",
    "$$\n",
    "\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "we substitute:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} \\bar{x}^2 = n \\bar{x}^2\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} x_i = n \\bar{x}\n",
    "$$\n",
    "\n",
    "so the equation simplifies to:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} (x_i - \\bar{x})^2 = \\sum_{i=1}^{n} x_i^2 - 2n\\bar{x}^2 + n\\bar{x}^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sum_{i=1}^{n} x_i^2 - n\\bar{x}^2\n",
    "$$\n",
    "\n",
    "#### **Step 3: Express in Terms of Mean of Squares**\n",
    "Dividing by $n - 1$:\n",
    "\n",
    "$$\n",
    "s^2 = \\frac{\\sum_{i=1}^{n} x_i^2 - n\\bar{x}^2}{n - 1}\n",
    "$$\n",
    "\n",
    "Rewriting using the **mean of squared values**:\n",
    "\n",
    "$$\n",
    "\\bar{x^2} = \\frac{1}{n} \\sum_{i=1}^{n} x_i^2\n",
    "$$\n",
    "\n",
    "we get:\n",
    "\n",
    "$$\n",
    "s^2 = \\frac{n}{n-1} (\\bar{x^2} - \\bar{x}^2)\n",
    "$$\n",
    "\n",
    "#### **Conclusion**\n",
    "Thus, we have derived the formula:\n",
    "\n",
    "$$\n",
    "s^2 = \\frac{n}{n-1} (\\bar{x^2} - \\bar{x}^2)\n",
    "$$\n",
    "\n",
    "This expresses the **sample variance** as a function of:\n",
    "- $\\bar{x^2}$: the mean of squared values, and\n",
    "- $\\bar{x}^2$: the square of the mean.\n",
    "\n",
    "The **$\\frac{n}{n-1}$ factor** (Bessel’s correction) ensures an **unbiased estimate** of the population variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some of the NumPy statistical functions that take ndarrays as an inputs.  With these methods if you had a multidimensional array you could calculate the average by row (axis = 1) or by column (axis = 0) or over the entire array (no axis specified). We just have a 1D ndarray so this is not applicable here.\n",
    "\n",
    "We calculate the inverse of the CDF, $F^{-1}_x(x)$ with Numpy percentile function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porosity = df['Porosity'].values\n",
    "\n",
    "print(f\"The minimum is {np.amin(porosity):.2f}.\")\n",
    "print(f\"The maximum is {np.amax(porosity):.2f}.\")\n",
    "print(f\"The range (maximum - minimum) is {np.ptp(porosity):.2f}.\")\n",
    "print(f\"The P10 is {np.percentile(porosity, 10):.3f}.\")\n",
    "print(f\"The P50 is {np.percentile(porosity, 50):.3f}.\")\n",
    "print(f\"The P90 is {np.percentile(porosity, 90):.3f}.\")\n",
    "print(f\"The P13 is {np.percentile(porosity, 13):.3f}.\")\n",
    "print(f\"The median (P50) is {np.median(porosity):.3f}.\")\n",
    "print(f\"The mean is {np.mean(porosity):.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the (Pearson) correlation matrix using `df.corr()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the CDF value, $F_x(x)$, directly from the data.\n",
    "* we use a condition to creat a boolean array with the same size of the data and then count the cases that meet the condition\n",
    "* we are assuming equal weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 0.10\n",
    "cumul_prob = np.count_nonzero(porosity <= value) / len(df)\n",
    "\n",
    "print(f\"The cumulative probability for porosity = {value:.2f} is {cumul_prob:.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Univariate Statistics\n",
    "\n",
    "Later we will talke about weights statistics. The NumPy command average allows for weighted averages as in the case of statistical expectation and declustered statistics. For demonstration, lets make a weighting array and apply it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = len(df)  # Get the number of data values\n",
    "wts = np.ones(nd)  # Create an array of ones for equal weighting\n",
    "\n",
    "equal_weighted_avg = np.average(porosity, weights=wts)\n",
    "\n",
    "print(f\"The equal-weighted average is {equal_weighted_avg:.3f}, the same as the mean above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for the **weighted average** is:\n",
    "\n",
    "$$\n",
    "\\bar{x}_{\\text{weighted}} = \\frac{\\sum_{i=1}^{n} x_i w_i}{\\sum_{i=1}^{n} w_i}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $x_i$ are the **porosity values**.\n",
    "- $w_i$ are the corresponding **weights** (`wts`).\n",
    "- $n$ is the **number of observations**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can modify the weights to be 0.5 if the porosity is greater than 13% and retain 1.0 if the porosity is less than or equal to 13%. The results should be a lower weighted average.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = np.ones_like(porosity)  # Create an array of ones with the same shape as porosity\n",
    "\n",
    "wts[porosity > 0.13] *= 0.5  # Reduce weights for porosity values greater than 0.13\n",
    "\n",
    "weighted_avg = np.average(porosity, weights=wts)\n",
    "\n",
    "print(f\"The weighted average is {weighted_avg:.3f}, lower than the equal-weighted average above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy stats functions provide a handy summary statistics function. The output is a 'list' of values (actually it is a SciPy.DescribeResult object). One can extract any one of them to use in a workflow as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics\n",
    "por_stats = stats.describe(df['Porosity'])\n",
    "\n",
    "# Print full summary statistics\n",
    "print(por_stats)\n",
    "\n",
    "# Extract and print kurtosis\n",
    "print(f\"Porosity kurtosis is {por_stats.kurtosis:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms\n",
    "\n",
    "Let's display some histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display histogram of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a histogram for porosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pormin, pormax = 0.05, 0.25\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(df['Porosity'].values,alpha=0.8,color=\"darkorange\",edgecolor=\"black\",bins=20,range=[pormin,pormax])\n",
    "plt.title('Histogram'); plt.xlabel('Porosity (fraction)'); plt.ylabel(\"Frequency\")\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.1, hspace=0.2); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks bimodal. \n",
    "\n",
    "#### Histogram Bins, Number of Bins and Bin Size\n",
    "\n",
    "Let's explore with a few bins sizes to check the impact on the histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start by creating a slightly modified plot function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what happens when we use:\n",
    "\n",
    "* **too large bins / too few bins** - often smooth out, removes information\n",
    "* **too small bins / too many bins** - often too noisy, obscures information  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized Histograms\n",
    "\n",
    "Normalized histograms are convienient since we can read relative frequency to be in each bin and observe closure by summing the relative frequency for all bins is 1.0.\n",
    "\n",
    "* to do this we need to explicity set the weight for each data as $\\frac{1}{n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.ones(len(df)) / len(df)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(porosity,alpha=0.8,color=\"darkorange\",edgecolor=\"black\",bins=25,range=[pormin,pormax],weights=weights)\n",
    "plt.title('Normalized Histogram'); plt.xlabel('Porosity (fraction)'); plt.ylabel(\"Prob\")\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.1, hspace=0.2); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel densities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's overlay a kernel on the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute normalized weights for the histogram\n",
    "weights = np.ones(len(df)) / len(df)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Create histogram\n",
    "plt.hist(porosity, alpha=0.8, color=\"darkorange\", edgecolor=\"black\", bins=25, \n",
    "         range=[pormin, pormax], weights=weights, density=True, label=\"\")\n",
    "\n",
    "# Compute and plot KDE\n",
    "kde = gaussian_kde(porosity, bw_method=0.1)  # Adjust bandwidth as needed\n",
    "x_vals = np.linspace(pormin, pormax, 200)  # Smooth x-axis for KDE curve\n",
    "plt.plot(x_vals, kde(x_vals), color='darkblue', linewidth=3, label=\"\")\n",
    "\n",
    "# Labels and title\n",
    "plt.title('Normalized Histogram with KDE', fontsize=16)\n",
    "plt.xlabel('Porosity (fraction)', fontsize=14)\n",
    "plt.ylabel(\"Probability Density\", fontsize=14)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.subplots_adjust(left=0.1, bottom=0.1, right=0.95, top=0.9, wspace=0.1, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Density Functions\n",
    "\n",
    "If we decide to use the sampled data to represent uncertainty, the practical way to calculate a probability density function (PDF) from data is to use of kernel density estimate (KDE).\n",
    "\n",
    "* we place a kernel, in this case a parametric Gaussian PDF, at each data value and then calculate the sum of all data kernels.\n",
    "* constrained for closure such that the area under the curve is 1.0.\n",
    "* differentiating the data CDF is usually too noisy to be useful.\n",
    "\n",
    "To demonstrate the KDE method, we calculate the KDE PDF for the first 2, 5, ..., 200 data. \n",
    "\n",
    "* when there are very few data you can see the individual Gaussian kernels\n",
    "* with more data they start to smooth out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set font sizes\n",
    "title_fontsize = 30  # Adjust title font size\n",
    "label_fontsize = 25  # Adjust x and y labels font size\n",
    "tick_fontsize = 22   # Adjust tick labels font size\n",
    "\n",
    "nums = [2, 4, 10, 20, 50, 200]\n",
    "\n",
    "# Define the range for the x-axis\n",
    "x_vals = np.linspace(0, 0.25, 200)  # 200 points between 0 and 0.25\n",
    "\n",
    "plt.figure(figsize=(15, 10))  # Adjust figure size for readability\n",
    "\n",
    "for i, num in enumerate(nums):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "\n",
    "    # Extract the first 'num' data points\n",
    "    porosity_subset = df['Porosity'].values[:num]\n",
    "\n",
    "    if len(porosity_subset) > 1:  # Ensure we have enough data for KDE\n",
    "        kde = gaussian_kde(porosity_subset, bw_method=0.1)  # Bandwidth = 0.1\n",
    "        plt.plot(x_vals, kde(x_vals), color='darkorange', linewidth=8, alpha=1.0)\n",
    "    \n",
    "    plt.xlim([0, 0.25])\n",
    "    plt.title(f'KDE PDF for First {num} Data', fontsize=title_fontsize)\n",
    "    plt.xlabel('Porosity (fraction)', fontsize=label_fontsize)\n",
    "    plt.ylabel(\"Density\", fontsize=label_fontsize)\n",
    "    \n",
    "    # Set tick font sizes\n",
    "    plt.xticks(fontsize=tick_fontsize)\n",
    "    plt.yticks(fontsize=tick_fontsize)\n",
    "\n",
    "# Adjust subplot layout\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=2.1, wspace=0.3, hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the impact of changing the kernel width on the KDE PDF model? \n",
    "\n",
    "* let's loop over a variety of kernel sizes and observe the resulting PDF with the data histogram.\n",
    "* note, kernel width is controlled by bandwidth, but the bandwidth parameter is poorly documented in Seaborn and seems to be related to original standard deviation. My hypothesis is the kernel standard deviation is the product of the bandwidth and the standard deviation of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bandwidth values\n",
    "bandwidths = [0.01, 0.05, 0.1, 0.3]\n",
    "\n",
    "# Create subplots\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "for i, bw in enumerate(bandwidths):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "\n",
    "    # Compute standard deviation of porosity\n",
    "    porosity_std = np.std(df['Porosity'])\n",
    "    \n",
    "    # Print bandwidth information\n",
    "    print(f'Bandwidth = {bw}, Bandwidth x Standard Deviation = {bw * porosity_std}')\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.hist(df['Porosity'].values, alpha=0.7, color=\"darkorange\", edgecolor=\"black\",\n",
    "             bins=25, range=[pormin, pormax], density=True, label=\"Histogram\")\n",
    "\n",
    "    # Compute and plot KDE using SciPy\n",
    "    kde = gaussian_kde(df['Porosity'].values, bw_method=bw)\n",
    "    x_vals = np.linspace(pormin, pormax, 200)  # Smooth x-axis for KDE curve\n",
    "    plt.plot(x_vals, kde(x_vals), color='black', alpha=0.8, linewidth=4.0, label=\"KDE\")\n",
    "\n",
    "    # Axis limits and labels\n",
    "    plt.xlim([0.0, 0.3])\n",
    "    plt.title(f'Histogram and KDE, BW = {bw}', fontsize=14)\n",
    "    plt.xlabel('Porosity (fraction)', fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "# Adjust subplot layout and show plot\n",
    "plt.subplots_adjust(left=0.05, bottom=0.05, right=0.95, top=0.95, wspace=0.3, hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bandwidth in Kernel Density Estimation (KDE)**\n",
    "\n",
    "#### **What Do These Calculations Tell Us?**\n",
    "\n",
    "When performing Kernel Density Estimation (KDE), the **bandwidth (`bw`)** parameter controls the level of **smoothing** applied to the estimated probability density function. The effect of bandwidth is influenced by the **spread of the data**, which is measured by the **standard deviation (`σ`)** of the dataset.\n",
    "\n",
    "In the calculations below, we analyze the impact of different bandwidth values:\n",
    "\n",
    "| Bandwidth (`bw`) | Bandwidth × Standard Deviation (`bw × σ`) |\n",
    "|-----------------|-------------------------------------|\n",
    "| 0.01           | 0.00050                             |\n",
    "| 0.05           | 0.00248                             |\n",
    "| 0.1            | 0.00497                             |\n",
    "| 0.3            | 0.01491                             |\n",
    "\n",
    "#### **Interpretation:**\n",
    "- **Bandwidth (`bw`) controls the smoothing level** of the KDE.\n",
    "- The **product `bw × σ`** represents the **absolute smoothing width**, indicating how much the KDE function is spread over the data.\n",
    "\n",
    "#### **Key Takeaways:**\n",
    "1. **For `bw = 0.01`:**  \n",
    "   - `bw × σ = 0.0005`\n",
    "   - Very small smoothing → KDE is **highly sensitive** to small variations, potentially capturing noise rather than the true distribution (**overfitting**).\n",
    "  \n",
    "2. **For `bw = 0.05`:**  \n",
    "   - `bw × σ = 0.0025`\n",
    "   - Still relatively narrow smoothing, capturing finer details but at risk of being too sensitive to small fluctuations.\n",
    "\n",
    "3. **For `bw = 0.1`:**  \n",
    "   - `bw × σ = 0.005`\n",
    "   - A more balanced smoothing effect, offering a compromise between detail and generalization.\n",
    "\n",
    "4. **For `bw = 0.3`:**  \n",
    "   - `bw × σ = 0.015`\n",
    "   - Very wide smoothing → KDE becomes much smoother, possibly missing important features (**underfitting**).\n",
    "\n",
    "#### **Practical Use:**\n",
    "- A **small bandwidth** makes the KDE **closer to a histogram**, revealing fine details but also amplifying noise.\n",
    "- A **large bandwidth** smooths the KDE into a **broad Gaussian-like curve**, which may lose key structural elements.\n",
    "- The **optimal bandwidth** depends on the **data distribution**. A common approach is to use **Silverman’s rule of thumb** or **cross-validation** to determine the best bandwidth automatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimal Bandwidth for Kernel Density Estimation (KDE)**\n",
    "\n",
    "The **optimal bandwidth** for a dataset, can be found by using **Silverman’s rule of thumb** or a **cross-validation-based approach**. Below, we compute and visualize the effect of different bandwidth choices.\n",
    "\n",
    "#### **Silverman’s Rule of Thumb**\n",
    "Silverman’s rule provides a default bandwidth estimate:\n",
    "\n",
    "$\n",
    "h = 1.06 \\cdot \\sigma \\cdot n^{-1/5}\n",
    "$\n",
    "\n",
    "where:\n",
    "- $\\sigma$ is the **standard deviation** of the data,\n",
    "- $n$ is the **number of observations**.\n",
    "\n",
    "This rule provides a well-balanced smoothing parameter that adapts to the spread and size of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the optimal bandwdith for KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute standard deviation and sample size\n",
    "sigma = np.std(porosity, ddof=1)  # Using sample standard deviation (unbiased)\n",
    "n = len(porosity)\n",
    "\n",
    "# Compute Silverman's optimal bandwidth\n",
    "silverman_bw = 1.06 * sigma * n ** (-1 / 5)\n",
    "\n",
    "# Display the optimal bandwidth value\n",
    "print(f\"Silverman's Optimal Bandwidth: {silverman_bw:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the optimal bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute Silverman's bandwidth\n",
    "sigma = np.std(porosity, ddof=1)  # Unbiased standard deviation\n",
    "n = len(porosity)\n",
    "silverman_bw = 1.06 * sigma * n ** (-1 / 5)\n",
    "\n",
    "# Compute normalized weights for histogram\n",
    "weights = np.ones(len(porosity)) / len(porosity)\n",
    "\n",
    "# Create histogram\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(porosity, alpha=0.8, color=\"darkorange\", edgecolor=\"black\", bins=25, \n",
    "         range=[pormin, pormax], weights=weights, density=True, label=\"\")\n",
    "\n",
    "# Compute and plot KDE\n",
    "kde = gaussian_kde(porosity, bw_method=silverman_bw)\n",
    "x_vals = np.linspace(pormin, pormax, 200)\n",
    "plt.plot(x_vals, kde(x_vals), color='darkblue', linewidth=3, label=\"\")\n",
    "\n",
    "# Labels and title\n",
    "plt.title('Normalized Histogram with KDE', fontsize=16)\n",
    "plt.xlabel('Porosity (fraction)', fontsize=14)\n",
    "plt.ylabel(\"Probability Density\", fontsize=14)\n",
    "\n",
    "# Add annotation for bandwidth in the upper left corner\n",
    "plt.annotate(f'Bandwidth: {silverman_bw:.6f}', xy=(0.03, 0.9), xycoords='axes fraction',\n",
    "             fontsize=14, color='black', bbox=dict(facecolor='white', edgecolor='white', boxstyle='round,pad=0.5'))\n",
    "\n",
    "# Add legend\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.subplots_adjust(left=0.1, bottom=0.1, right=0.95, top=0.9, wspace=0.1, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Distribution Functions\n",
    "\n",
    "* the y axis is cumulative probability with a minimum of 0.0 and maximum of 1.0 as expected for a CDF.\n",
    "* note after the initial hist command we can add a variety of elements such as labels to our plot as shown below.\n",
    "* we can increase or decrease the number of bins, $> n$ is data resolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(porosity,density=True, cumulative=True, label='CDF',\n",
    "           histtype='stepfilled', alpha=0.8, bins = 100, color='darkorange', edgecolor = 'black', range=[0.0,0.25])\n",
    "plt.xlabel('Porosity (fraction)')\n",
    "plt.title('Porosity CDF')\n",
    "plt.ylabel('Cumulation Probability')\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.0, wspace=0.1, hspace=0.2)\n",
    "#plt.savefig('cdf_Porosity.tif',dpi=600,bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating and Plotting a CDF 'by- Hand'\n",
    "\n",
    "Let's demonstrate the calculation and plotting of a non-parametric CDF by hand\n",
    "\n",
    "1. make a copy of the feature as a 1D array (ndarray from NumPy)\n",
    "2. sort the data in ascending order\n",
    "3. assign cumulative probabilities based on the tail assumptions\n",
    "4. plot cumuative probability vs. value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "por = df['Porosity'].copy(deep = True).values # make a deepcopy of the feature from the DataFrame\n",
    "print('The ndarray has a shape of ' + str(por.shape) + '.')\n",
    "\n",
    "por = np.sort(por)                           # sort the data in ascending order\n",
    "n = por.shape[0]                             # get the number of data samples\n",
    "\n",
    "cprob = np.zeros(n)\n",
    "for i in range(0,n):\n",
    "    index = i + 1\n",
    "    cprob[i] = index / n                     # known upper tail\n",
    "    # cprob[i] = (index - 1)/n               # known lower tail\n",
    "    # cprob[i] = (index - 1)/(n - 1)         # known upper and lower tails\n",
    "    # cprob[i] = index/(n+1)                 # unknown tails  \n",
    "\n",
    "plt.subplot(111)\n",
    "plt.plot(por,cprob, alpha = 0.8, c = 'black',zorder=1) # plot piecewise linear interpolation\n",
    "plt.scatter(por,cprob,s = 20, alpha = 1.0, c = 'darkorange', edgecolor = 'black',zorder=2) # plot the CDF points\n",
    "plt.grid(); plt.xlim([0.05,0.25]); plt.ylim([0.0,1.0])\n",
    "plt.xlabel(\"Porosity (fraction)\"); plt.ylabel(\"Cumulative Probability\"); plt.title(\"Cumulative Distribution Function\")\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.1, wspace=0.1, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Probability Equations\n",
    "\n",
    "Different formulas are used to compute cumulative probabilities based on tail assumptions.\n",
    "\n",
    "##### **Known Upper Tail (Default)**\n",
    "$$\n",
    "cprob[i] = \\frac{i + 1}{n}\n",
    "$$\n",
    "- Assumes the dataset's **upper tail is known**.\n",
    "- The last data point has **cumulative probability = 1**.\n",
    "- Commonly used in **empirical CDFs**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Known Lower Tail**\n",
    "$$\n",
    "cprob[i] = \\frac{i}{n}\n",
    "$$\n",
    "- Assumes the dataset’s **lower tail is known**.\n",
    "- The first data point starts at **0** instead of **$1/n$**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Known Upper and Lower Tails**\n",
    "$$\n",
    "cprob[i] = \\frac{i}{n - 1}\n",
    "$$\n",
    "- Adjusts for both **upper and lower tails**.\n",
    "- Distributes probabilities more evenly.\n",
    "- Ensures the **last value reaches 1 exactly**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Unknown Tails (Cunnane Plotting Position)**\n",
    "$$\n",
    "cprob[i] = \\frac{i + 1}{n + 1}\n",
    "$$\n",
    "- **Adjusts for unknown tails** by using $n+1$.\n",
    "- Used in **probability plotting positions** (e.g., **Cunnane's method**).\n",
    "- **Ensures values never reach 0 or 1 exactly**, useful for interpolation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Table**\n",
    "| **Method** | **Formula** | **Assumptions** |\n",
    "|------------|------------|----------------|\n",
    "| **Known Upper Tail** | $ \\frac{i + 1}{n} $ | Last value = 1 (default) |\n",
    "| **Known Lower Tail** | $ \\frac{i}{n} $ | First value = 0 |\n",
    "| **Known Upper & Lower Tails** | $ \\frac{i}{n - 1} $ | Uses $n-1$ for scaling |\n",
    "| **Unknown Tails (Cunnane)** | $ \\frac{i + 1}{n + 1} $ | Ensures values stay within (0,1) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large datasets, such as the one we are working with, the resulting CDFs using the different formulas is not usually significant and can be ignored. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test using a smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 1. LOAD & SORT THE DATA\n",
    "# -----------------------------\n",
    "# (Assuming 'df' is your DataFrame containing the 'Porosity' column)\n",
    "por = df['Porosity'].copy(deep=True).values  \n",
    "#print(f'The ndarray has a shape of {por.shape}.')  # e.g., (261,)\n",
    "\n",
    "# Sort the data\n",
    "por = np.sort(por)\n",
    "n_full = por.shape[0]\n",
    "\n",
    "# -----------------------------\n",
    "# 2. OPTIONALLY SAMPLE THE DATA\n",
    "# -----------------------------\n",
    "use_all_samples = False  # Change to False to use only a subset\n",
    "n_samples = 20          # Only used if use_all_samples is False\n",
    "\n",
    "if use_all_samples:\n",
    "    porosity_sampled = por\n",
    "    indices = np.arange(n_full)  # all indices\n",
    "else:\n",
    "    indices = np.linspace(0, n_full - 1, n_samples, dtype=int)\n",
    "    porosity_sampled = por[indices]\n",
    "\n",
    "# -----------------------------\n",
    "# 3. COMPUTE FOUR CDF VARIATIONS\n",
    "# -----------------------------\n",
    "# We'll compute these for the full dataset then sample if needed.\n",
    "cprob_upper   = np.zeros(n_full)   # Known Upper Tail: (i+1)/n\n",
    "cprob_lower   = np.zeros(n_full)   # Known Lower Tail: i/n\n",
    "cprob_both    = np.zeros(n_full)   # Known Upper & Lower Tails: i/(n-1)\n",
    "cprob_unknown = np.zeros(n_full)   # Unknown Tails (Cunnane-like): (i+1)/(n+1)\n",
    "\n",
    "for i in range(n_full):\n",
    "    idx = i + 1  # using 1-based indexing for the formulas\n",
    "    cprob_upper[i]   = idx / n_full\n",
    "    cprob_lower[i]   = (idx - 1) / n_full\n",
    "    cprob_both[i]    = (idx - 1) / (n_full - 1)\n",
    "    cprob_unknown[i] = idx / (n_full + 1)\n",
    "\n",
    "# Downsample the CDF arrays if not using all samples\n",
    "if not use_all_samples:\n",
    "    cprob_upper   = cprob_upper[indices]\n",
    "    cprob_lower   = cprob_lower[indices]\n",
    "    cprob_both    = cprob_both[indices]\n",
    "    cprob_unknown = cprob_unknown[indices]\n",
    "\n",
    "# -----------------------------\n",
    "# 4. COMPUTE DIFFERENCES (relative to Known Upper Tail)\n",
    "# -----------------------------\n",
    "# These differences are small (order ~1/n) but we can plot them for clarity.\n",
    "diff_lower   = cprob_upper - cprob_lower\n",
    "diff_both    = cprob_upper - cprob_both\n",
    "diff_unknown = cprob_upper - cprob_unknown\n",
    "\n",
    "# -----------------------------\n",
    "# 5. PLOT THE RESULTS\n",
    "# -----------------------------\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
    "\n",
    "# Top subplot: the four CDF curves\n",
    "axs[0].plot(porosity_sampled, cprob_upper,   label=\"Known Upper Tail\",       color='blue')\n",
    "axs[0].plot(porosity_sampled, cprob_lower,   label=\"Known Lower Tail\",       color='green')\n",
    "axs[0].plot(porosity_sampled, cprob_both,    label=\"Known Upper & Lower\",    color='red')\n",
    "axs[0].plot(porosity_sampled, cprob_unknown, label=\"Unknown Tails (Cunnane)\", color='orange')\n",
    "axs[0].set_ylabel(\"Cumulative Probability\")\n",
    "axs[0].set_title(\"Empirical CDFs with Different Tail Conventions\")\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "axs[0].set_xlim([0.05, 0.25])\n",
    "axs[0].set_ylim([0, 1])\n",
    "\n",
    "# Bottom subplot: differences relative to the Known Upper Tail\n",
    "axs[1].plot(porosity_sampled, diff_lower,   label=\"Upper - Lower\",   marker='o', color='green')\n",
    "axs[1].plot(porosity_sampled, diff_both,    label=\"Upper - Both\",    marker='s', color='red')\n",
    "axs[1].plot(porosity_sampled, diff_unknown, label=\"Upper - Unknown\", marker='^', color='orange')\n",
    "axs[1].set_xlabel(\"Porosity (fraction)\")\n",
    "axs[1].set_ylabel(\"Difference\")\n",
    "axs[1].set_title(\"Differences Relative to Known Upper Tail\")\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "# Adjust y-axis limits to zoom in on the small differences:\n",
    "# The maximum difference is roughly ~1/n_full. For n_full=261, that’s about 0.0038.\n",
    "axs[1].set_ylim([-0.001, 0.005])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In conclusion, let's finish with the histograms of all of our features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define min/max values for the variables\n",
    "permmin, permmax = 0.01, 3000\n",
    "AImin, AImax = 1000.0, 8000\n",
    "Fmin, Fmax = 0, 1\n",
    "pormin, pormax = 0.05, 0.25  # Adjusted based on the observed histogram\n",
    "\n",
    "# Create figure and subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Facies Histogram\n",
    "axes[0, 0].hist(df['Facies'].values, bins=20, range=[Fmin, Fmax], \n",
    "                color=\"darkorange\", edgecolor=\"black\", alpha=0.8)\n",
    "axes[0, 0].set_title('Facies Well Data', fontsize=14)\n",
    "axes[0, 0].set_xlabel('Facies (1-sand, 0-shale)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "\n",
    "# Porosity Histogram\n",
    "axes[0, 1].hist(df['Porosity'].values, bins=20, range=[pormin, pormax], \n",
    "                color=\"darkorange\", edgecolor=\"black\", alpha=0.8)\n",
    "axes[0, 1].set_title('Porosity Well Data', fontsize=14)\n",
    "axes[0, 1].set_xlabel('Porosity (fraction)', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=12)\n",
    "\n",
    "# Permeability Histogram\n",
    "axes[1, 0].hist(df['Perm'].values, bins=20, range=[permmin, permmax], \n",
    "                color=\"darkorange\", edgecolor=\"black\", alpha=0.8)\n",
    "axes[1, 0].set_title('Permeability Well Data', fontsize=14)\n",
    "axes[1, 0].set_xlabel('Permeability (mD)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
    "\n",
    "# Acoustic Impedance Histogram\n",
    "axes[1, 1].hist(df['AI'].values, bins=20, range=[AImin, AImax], \n",
    "                color=\"darkorange\", edgecolor=\"black\", alpha=0.8)\n",
    "axes[1, 1].set_title('Acoustic Impedance Well Data', fontsize=14)\n",
    "axes[1, 1].set_xlabel('Acoustic Impedance (kg/m2s*10^6)', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "\n",
    "# Adjust layout for clarity\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "\n",
    "This was a basic demonstration of calculating univariate statistics and visualizing data distributions. Much more could be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "require": {
   "paths": {
    "buttons.colvis": "https://cdn.datatables.net/buttons/1.5.6/js/buttons.colVis.min",
    "buttons.flash": "https://cdn.datatables.net/buttons/1.5.6/js/buttons.flash.min",
    "buttons.html5": "https://cdn.datatables.net/buttons/1.5.6/js/buttons.html5.min",
    "buttons.print": "https://cdn.datatables.net/buttons/1.5.6/js/buttons.print.min",
    "chartjs": "https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.8.0/Chart",
    "d3": "https://d3js.org/d3.v5.min",
    "d3-array": "https://d3js.org/d3-array.v2.min",
    "datatables.net": "https://cdn.datatables.net/1.10.18/js/jquery.dataTables",
    "datatables.net-buttons": "https://cdn.datatables.net/buttons/1.5.6/js/dataTables.buttons.min",
    "datatables.responsive": "https://cdn.datatables.net/responsive/2.2.2/js/dataTables.responsive.min",
    "datatables.scroller": "https://cdn.datatables.net/scroller/2.0.0/js/dataTables.scroller.min",
    "datatables.select": "https://cdn.datatables.net/select/1.3.0/js/dataTables.select.min",
    "jszip": "https://cdnjs.cloudflare.com/ajax/libs/jszip/2.5.0/jszip.min",
    "moment": "https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.8.0/moment",
    "pdfmake": "https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.1.36/pdfmake.min",
    "vfsfonts": "https://cdnjs.cloudflare.com/ajax/libs/pdfmake/0.1.36/vfs_fonts"
   },
   "shim": {
    "buttons.colvis": {
     "deps": [
      "jszip",
      "datatables.net-buttons"
     ]
    },
    "buttons.flash": {
     "deps": [
      "jszip",
      "datatables.net-buttons"
     ]
    },
    "buttons.html5": {
     "deps": [
      "jszip",
      "datatables.net-buttons"
     ]
    },
    "buttons.print": {
     "deps": [
      "jszip",
      "datatables.net-buttons"
     ]
    },
    "chartjs": {
     "deps": [
      "moment"
     ]
    },
    "datatables.net": {
     "exports": "$.fn.dataTable"
    },
    "datatables.net-buttons": {
     "deps": [
      "datatables.net"
     ]
    },
    "pdfmake": {
     "deps": [
      "datatables.net"
     ]
    },
    "vfsfonts": {
     "deps": [
      "datatables.net"
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
