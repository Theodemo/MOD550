# üìö Table des Mati√®res ‚Äì Formules de Statistique

## 1. Statistiques descriptives

### 1.1. Mesures de tendance centrale

#### ‚Ä¢ Moyenne arithm√©tique simple 

$$\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$$

- $x_i$  : les valeurs de la s√©rie
- $n$ : le nombre total de donn√©es

---

#### ‚Ä¢ Moyenne pond√©r√©e

$$\bar{x}_p = \frac{\sum_{i=1}^{n} x_i \cdot w_i}{\sum_{i=1}^{n} w_i}$$

- $x_i$ : les valeurs
- $w_i$ : les poids ou effectifs associ√©s √† chaque valeur

---

#### ‚Ä¢ M√©diane

Si $n$ est impair :

$$M = x_{\frac{n+1}{2}}$$ 

Si $n$ est pair : 

$$M = \frac{x_{\frac{n}{2}} + x_{\frac{n}{2} + 1}}{2}$$  

- $M$ : M√©diane (valeur centrale)
- $n$ : Nombre total de donn√©es
- $x_i$ : Valeurs tri√©es de l'√©chantillon

---

#### ‚Ä¢ Mode

$$ \text{Mode} = \arg\max_x f(x)$$ 

- Mode : Valeur la plus fr√©quente dans l‚Äô√©chantillon
- $f(x)$ : Fr√©quence de $x$

---

### 1.2. Mesures de dispersion

#### ‚Ä¢ √âtendue

$$\text{√âtendue} = x_{\text{max}} - x_{\text{min}}$$

- $x_max$ : valeur maximale
- $x_min$ : valeur minimale

---

#### ‚Ä¢ Variance

$$s^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2$$

- $x_i$ : les valeurs de la s√©rie
- $\bar{x}$ : la moyenne
- $n$ : nombre total de donn√©es

---

#### ‚Ä¢ √âcart-type

$$\sigma = \sqrt{s^2}$$

- Racine carr√©e de la variance  
- Permet de mesurer l‚Äô**√©cart moyen √† la moyenne**

---

#### ‚Ä¢ √âcart interquartile

$$IQR = Q_3 - Q_1$$

- $Q_1$ : premier quartile (25 % des donn√©es)
- $Q_3$ : troisi√®me quartile (75 % des donn√©es)

---

#### ‚Ä¢ Coefficient de variation


$$CV = \frac{\sigma}{\bar{x}} \times 100$$

- $œÉ$ : √©cart-type
- $\bar{x}$ : moyenne
- Exprim√© en **%**, il permet de comparer la dispersion entre s√©ries

---

### 1.3. Mesures de position

#### ‚Ä¢ Quartiles

- $Q1$ : 25 % des donn√©es sont inf√©rieures ou √©gales √† cette valeur  
- $Q2$ : m√©diane (50 %)  
- $Q3$ : 75 % des donn√©es sont inf√©rieures ou √©gales √† cette valeur
  
#### ‚Ä¢ D√©ciles

- $D1$ √† $D9$ : divisent la s√©rie en 10 parties √©gales  
  Ex : $D4$ = 40 % des donn√©es ‚â§ D4

---

#### ‚Ä¢ Centiles

- $P1$ √† $P99$ : divisent la s√©rie en 100 parties √©gales  
  Ex : $P90$ = 90 % des donn√©es ‚â§ P90

---

### 1.4. Tableaux statistiques

#### ‚Ä¢ Fr√©quence absolue

$$f_i = \text{nombre d‚Äôoccurrences de la valeur } x_i$$

#### ‚Ä¢ Fr√©quence relative

$$f_i^{\text{rel}} = \frac{f_i}{n}$$

- $f_i$ : fr√©quence absolue
- $n$ : total des donn√©es

---

#### ‚Ä¢ Fr√©quence cumul√©e croissante

$$F_i = \sum_{j=1}^{i} f_j^{\text{rel}}$$

- Permet de conna√Ætre le pourcentage de donn√©es **inf√©rieures ou √©gales** √† une valeur donn√©e.

---

#### ‚Ä¢ Effectifs en classes (donn√©es group√©es)

- On regroupe les valeurs dans des **intervalles** (ou classes)
- Pour chaque classe, on calcule :
  - **Effectif de classe** : nombre d‚Äôobservations dans la classe
  - **Centre de classe** : $c = \frac{\text{borne inf√©rieure + borne sup√©rieure}}{2}$

---

## 2. Statistiques √† deux variables

### 2.1. Covariance
#### Formule de la covariance  
#### Interpr√©tation graphique  

### 2.2. Corr√©lation
#### Coefficient de corr√©lation lin√©aire de Pearson  
#### Valeurs possibles et interpr√©tation  

### 2.3. R√©gression lin√©aire
#### √âquation de la droite de r√©gression : `y = ax + b`  
#### Calcul des coefficients `a` et `b`  
#### Coefficient de d√©termination `R¬≤`

---

## 3. Variables al√©atoires et esp√©rance

### 3.1. Loi de probabilit√© discr√®te
#### Fonction de probabilit√©  
#### Esp√©rance math√©matique  
#### Variance et √©cart-type  

### 3.2. Lois usuelles
#### Loi uniforme discr√®te  
#### Loi de Bernoulli  
#### Loi binomiale

$$P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}$$

- $P(X = k)$ : Probabilit√© d‚Äôobtenir exactement $k$ succ√®s
- $n$ : Nombre d‚Äôessais
- $k$ : Nombre de succ√®s souhait√©s
- $p$ : Probabilit√© de succ√®s dans un essai
- $\binom{n}{k}$ : Coefficient binomial

---

#### Loi normale

$$ f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}$$

- $f(x)$ : Fonction de densit√© de probabilit√© de la loi normale
- $x$ : Valeur de la variable al√©atoire
- $\mu$ : Moyenne
- $\sigma$ : √âcart-type

---

#### Loi de Poisson

$$P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}$$

- $P(X = k)$ : Probabilit√© d‚Äôavoir exactement $k$ √©v√©nements
- $\lambda$ : Taux moyen d'√©v√©nements dans un intervalle
- $k$ : Nombre d'√©v√©nements
- $k!$ : Factorielle de $k$

---

#### Loi g√©om√©trique

---

## 4. Estimation et intervalles

### 4.1. Estimation ponctuelle
#### Moyenne, proportion, variance d‚Äôun √©chantillon

### 4.2. Intervalle de confiance
#### Pour une moyenne (`œÉ` connue)  
#### Pour une proportion  
#### Taille d‚Äôun √©chantillon n√©cessaire

---

## 5. Tests statistiques (niveau avanc√©)

### 5.1. Hypoth√®ses
#### Hypoth√®se nulle `H‚ÇÄ` et alternative `H‚ÇÅ`  
#### Risque d‚Äôerreur de type I et II

### 5.2. Test de moyenne / proportion
#### Z-test  
#### T-test (`œÉ` inconnue)

### 5.3. Khi¬≤
#### Test d‚Äôind√©pendance  
#### Test d‚Äôajustement

---

## 6. Compl√©ments (niveau licence et plus)
#### Analyse de variance (ANOVA)  
#### R√©gression multiple  
#### Statistiques inf√©rentielles  
#### Bootstrap (estimation par r√©√©chantillonnage)



### **Coefficient de variation**  
$$CV = \frac{\sigma}{\bar{x}} \times 100\%$$  
- \( CV \) : Coefficient de variation
- \( \sigma \) : √âcart-type
- \( \bar{x} \) : Moyenne de l'√©chantillon


## **2. Probabilit√©s**

### **Probabilit√© d‚Äôun √©v√©nement**  
$$P(A) = \frac{\text{nombre de cas favorables}}{\text{nombre de cas possibles}}$$  
- \( P(A) \) : Probabilit√© de l'√©v√©nement \( A \)
- Nombre de cas favorables : Nombre de r√©sultats o√π \( A \) se produit
- Nombre de cas possibles : Total des r√©sultats possibles

### **Probabilit√© conditionnelle**  
$$P(A | B) = \frac{P(A \cap B)}{P(B)}$$  
- \( P(A | B) \) : Probabilit√© de \( A \) sachant que \( B \) s'est produit
- \( P(A \cap B) \) : Probabilit√© que \( A \) et \( B \) se produisent
- \( P(B) \) : Probabilit√© de \( B \)

### **Th√©or√®me de Bayes**  
$$P(A | B) = \frac{P(B | A) P(A)}{P(B)}$$  
- \( P(A | B) \) : Probabilit√© de \( A \) sachant \( B \)
- \( P(B | A) \) : Probabilit√© de \( B \) sachant \( A \)
- \( P(A) \) : Probabilit√© de \( A \)
- \( P(B) \) : Probabilit√© de \( B \)

---

## **3. Variables Al√©atoires et Distributions**

### **Esp√©rance math√©matique**  
$$E(X) = \sum x_i P(x_i)$$  
- \( E(X) \) : Esp√©rance de la variable al√©atoire \( X \)
- \( x_i \) : Valeurs possibles de \( X \)
- \( P(x_i) \) : Probabilit√© associ√©e √† \( x_i \)

### **Variance d‚Äôune variable al√©atoire**  
$$Var(X) = E(X^2) - (E(X))^2 $$  
- \( Var(X) \) : Variance de \( X \)
- \( E(X^2) \) : Esp√©rance de \( X^2 \)
- \( E(X) \) : Esp√©rance de \( X \)





## **4. Inf√©rence Statistique**

### **Intervalle de confiance pour une moyenne**  
$$IC = \bar{x} \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}} $$  
- \( IC \) : Intervalle de confiance pour la moyenne
- \( \bar{x} \) : Moyenne de l‚Äô√©chantillon
- \( z_{\alpha/2} \) : Valeur critique de la distribution normale
- \( \sigma \) : √âcart-type de l‚Äô√©chantillon
- \( n \) : Taille de l‚Äô√©chantillon

### **Test d‚Äôhypoth√®se**  
- \( H_0 \) : Hypoth√®se nulle
- \( H_1 \) : Hypoth√®se alternative  
Utilise une statistique de test (calcul√©e √† partir des donn√©es) pour comparer avec un seuil critique afin de d√©cider si \( H_0 \) est rejet√©e.

### **Test t de Student**  
$$ t = \frac{\bar{x} - \mu}{\frac{s}{\sqrt{n}}}$$  
- \( t \) : Statistique de test
- \( \bar{x} \) : Moyenne de l‚Äô√©chantillon
- \( \mu \) : Moyenne hypoth√©tique de la population
- \( s \) : √âcart-type de l‚Äô√©chantillon
- \( n \) : Taille de l‚Äô√©chantillon

### **Test du Chi-2**  
$$ \chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}$$  
- \( \chi^2 \) : Statistique de test du chi-deux
- \( O_i \) : Observations (fr√©quences observ√©es)
- \( E_i \) : Fr√©quences attendues

---

## **5. R√©gression et Corr√©lation**

### **Corr√©lation lin√©aire (coefficient de Pearson)**  
$$ r = \frac{\sum (x_i - \bar{x}) (y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}}$$  
- \( r \) : Coefficient de corr√©lation de Pearson
- \( x_i \), \( y_i \) : Valeurs des variables \( X \) et \( Y \)
- \( \bar{x} \), \( \bar{y} \) : Moyennes respectives de \( X \) et \( Y \)

### **R√©gression lin√©aire simple**  
$$ y = ax + b $$  
- \( y \) : Variable d√©pendante
- \( x \) : Variable ind√©pendante
- \( a \) : Pente de la r√©gression
- \( b \) : Ordonn√©e √† l'origine (intercept)

---

## **6. Statistiques Avanc√©es et Applications**

### **Analyse en composantes principales (ACP)**  
$$Z = XW $$  
- \( Z \) : Matrice des composantes principales
- \( X \) : Matrice des donn√©es originales
- \( W \) : Matrice des vecteurs propres

### **Clustering (K-means)**  
$$J = \sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2$$  
- \( J \) : Fonction de co√ªt (distances intra-cluster)
- \( C_i \) : Cluster \( i \)
- \( \mu_i \) : Centre du cluster \( i \)
- \( x \) : Donn√©es dans le cluster

### **R√©gression logistique**  
$$P(Y=1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}$$  
- \( P(Y=1 | X) \) : Probabilit√© que \( Y \) soit √©gal √† 1 pour une valeur de \( X \)
- \( \beta_0 \), \( \beta_1 \) : Coefficients de r√©gression
- \( X \) : Variable ind√©pendante

### **S√©ries temporelles (ARIMA)**  
$$Y_t = \alpha + \sum \phi_i Y_{t-i} + \sum \theta_j \varepsilon_{t-j} + \varepsilon_t$$  
- \( Y_t \) : Valeur de la s√©rie temporelle √† l'instant \( t \)
- \( \alpha \) : Constante
- \( \phi_i \) : Coefficients autor√©gressifs
- \( \varepsilon_t \) : R√©sidus (bruit al√©atoire)
